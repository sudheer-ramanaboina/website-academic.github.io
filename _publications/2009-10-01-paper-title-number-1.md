---
title: "LLM Models Survey"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
# excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2024-02-20
venue: 'Paper 1'
paperurl: /files/llm.pdf
citation: '(2024). &quot;Paper.&quot; <i>Paper</i>. 1.'
---

Large Language Models (LLMs) have drawn a
lot of attention due to their strong performance on a wide
range of natural language tasks, since the release of ChatGPT
in November 2022. LLMs’ ability of general-purpose language
understanding and generation is acquired by training billions of
model’s parameters on massive amounts of text data, as predicted
by scaling laws [1], [2]. The research area of LLMs, while very
recent, is evolving rapidly in many different ways. In this paper,
we review some of the most prominent LLMs, including three
popular LLM families (GPT, LLaMA, PaLM), and discuss their
characteristics, contributions and limitations. We also give an
overview of techniques developed to build, and augment LLMs.
We then survey popular datasets prepared for LLM training,
fine-tuning, and evaluation, review widely used LLM evaluation
metrics, and compare the performance of several popular LLMs
on a set of representative benchmarks. Finally, we conclude
the paper by discussing open challenges and future research
directions.
<!-- The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font. -->
